---
title:    "ISE 5103 Intelligent Data Analytics"
subtitle: "Final Project"
author:   "Daniel Carpenter, Sonaxy Mohanty, & Zachary Knepp"
date:     "December 2022"
output: 
  pdf_document:
    toc: true
    number_sections: true
    toc_depth: 2
    highlight: arrow
    latex_engine: xelatex
  # github_document:
  #   toc: yes
  #   toc_depth: 2
urlcolor: blue
cache: true
fig.width: 7
fig.height: 5
---

```{r setup, include=FALSE}            
                                       # By default...
knitr::opts_chunk$set(echo    = FALSE, # do not show code
                      message = FALSE, # Do not show messages
                      warning = FALSE, # Do not show warning messages
                      
                      # Figures
                      fig.width  = 6,
                      fig.height = 4, 
                      fig.align  = 'center'
                      )
```


```{r error=FALSE}
# Packages --------

# Data Wrangling
library(tidyverse)
library(skimr)
library(lubridate) # dates

# Modeling
library(MASS)
library(caret) # Modeling variants like SVM
library(earth) # Modeling with Mars
library(pls)   # Modeling with PLS
library(glmnet) # Modeling with LASSO

# Imputation
library(VIM)   # Factor: kNN
library(mice)  # Numeric: predictive mean matching

# skewness 
library(moments)
library(EnvStats) # "boxcox" function


# Aesthetics
library(knitr)
library(cowplot)  # multiple ggplots on one plot with plot_grid()
library(scales)
library(kableExtra)
library(inspectdf)

#Hold-out Validation
library(caTools)

#Data Correlation
library(GGally)
library(regclass)

#RMSE Calculation
library(Metrics)

#p-value for OLS model
library(broom)

#ncvTest
library(car)

library(corrplot)
```

\newpage

# General Data Prep
> For general data preparation, please see conceptual steps below. See `.rmd` file for detailed code.

## Creating the CSV Dataset
* Note that the original training and test data [found here](https://www.kaggle.com/datasets/radadiyamohit/time-taken-by-delivery-person)
contains two zipped files totaling around 55,000 `.txt` files  
* In order to convert this data into a usable format, we created a function that:  
  - Reads all `txt` contained within a specified folder  
  - Cleans whitespace, variable naming conventions, and converts `Time_Ordered` 
  and `Time_Ordered_Picked` from HH:MM string time.  
  - All variables are cast to their correct data types  
  - Finally, the data is exported to a single CSV.  
  - This function is applied to to the training and test data  
  - [This R file containing the function is located here](https://github.com/Daniel-Carpenter/IDA-Final-Project/blob/main/2%20-%20Initial%20Data%20Analysis/ReadAndCleanTxtFiles.R)  
* Note that the function is not run within this file due to the time required to run the code. Since there are so many files, it takes a large amount of time.  


## Read Training and Test Data
* Read [training](https://raw.githubusercontent.com/Daniel-Carpenter/IDA-Final-Project/main/2%20-%20Initial%20Data%20Analysis/TrainingData.csv) and [test](https://raw.githubusercontent.com/Daniel-Carpenter/IDA-Final-Project/main/2%20-%20Initial%20Data%20Analysis/TestData.csv) data CSV files from GitHub  
* Clean data to ensure each read variable has the correct data type (factor, numeric, Date, etc.)  

```{r, cache=TRUE}
# Read in Data -----------------------------------------------------------------

# Function to read from Git and convert variable types
readCleanFromGit <- function(gitDirectoryURL, csvName, isTrainingData) {
  
  ## GitHub URL's
  csvFullURL <- paste0(gitDirectoryURL, csvName)
  
  ## Read the data from GitHub
  df <- read.csv(csvFullURL, stringsAsFactors = TRUE) # Training data
  
  # Ensure Type Conversion -----------------------------------------------------
  
  ## Convert all character data to factor
  df.goodTypes <- df %>% 
    
    ### Ensure boolean variables are numeric
    mutate_at(vars(
      Delivery_Person_Age,
      Delivery_Person_Ratings,
      Restaurant_Latitude,
      Restaurant_Longitude,
      Delivery_Location_Latitude,
      Delivery_Location_Longitude,
      Restaurant_Longitude,
      Time_Ordered,
      Time_Order_Picked
      ), as.numeric) %>%
    
    ### Make sure dates are dates
    mutate_at(vars(Order_Date), as.Date) %>%
  
    ### Ensure factor are factors
    mutate_at(vars(
      Id,
      Delivery_Person_Id,
      Weather_Conditions,
      Road_Traffic_Density,
      Vehicle_Condition,
      Type_Of_Order,
      Type_Of_Vehicle,
      Multiple_Deliveries,
      City,
      Name
    ), as.factor) %>%
    
    mutate(Festival = as.factor(if_else(is.na(Festival), 0, 1)) )
      
  # If training data then ensure target variable is good.
  if (isTrainingData) {
    df.goodTypes <- df.goodTypes %>%
      mutate(Target_Variable = as.numeric(Target_Variable) )
  }
  
  # Return csv with type conversion
  return(df.goodTypes)
} 


# Actually read in the data and clean from Git ---------------------------------
gitDirectory <- 'https://raw.githubusercontent.com/Daniel-Carpenter/IDA-Final-Project/main/2%20-%20Initial%20Data%20Analysis/'


## Training data
df.train.base <- readCleanFromGit(gitDirectoryURL = gitDirectory, 
                                 csvName         = 'TrainingData.csv',
                                 isTrainingData  = TRUE)

## Test data
df.test.base  <- readCleanFromGit(gitDirectoryURL = gitDirectory, 
                                  csvName         = 'TestData.csv',
                                  isTrainingData  = FALSE)

# Test to see if type conversion worked
# lapply(df.train.base, class)
# lapply(df.test.base, class)
```


## Create `numeric` and `factor` *base* `data frames`
```{r}
# Function to separate into factor and numeric data ----------------------------
separateFactorAndNumeric <- function(df, functionType) {
  
  # Numeric Data frame
  df.typeSubset<- df %>%
  
    # selecting all the ______ data, e.g. is.numeric
    dplyr::select_if(functionType) %>%
  
    # converting the data frame to tibble
    as_tibble()
  
  return(df.typeSubset)
}


# Create the factor and numeric data -------------------------------------------

## Training data
df.train.base.numeric <- separateFactorAndNumeric(df.train.base, is.numeric) # numeric
df.train.base.factor  <- separateFactorAndNumeric(df.train.base, is.factor)  # factor

## Test Data
df.test.base.numeric  <- separateFactorAndNumeric(df.train.base, is.numeric) # numeric
df.test.base.factor   <- separateFactorAndNumeric(df.train.base, is.factor)  # factor
```

\newpage

# Data Understanding
> Create a data quality report of `numeric` and `factor` data  
> Created function called `dataQualityReport()` to create factor and numeric QA report

```{r}
# Function for data report
dataQualityReport <- function(df) {
  
  # Function to remove any columns with NA
  removeColsWithNA <- function(df) {
    return( df[ , colSums(is.na(df)) == 0] )
  }
  
  # Create Comprehensive data report using skimr package
  # This is done a bit piece-wise because PDF latex does not like the skimr package
  # Very much. So Instead of printing `skim(df)`, I have to pull the contents manually
  # Unfortunately. This is not an issue with html typically.
  dataReport <- skim(df) %>%
    rename_all(~str_replace(.,"skim_","")) %>%
    arrange(type, desc(complete_rate) ) # sort data 
  
  # Filter to the class types
  dataReport.numeric <- dataReport %>% filter(type == 'numeric') # numeric data
  dataReport.factor  <- dataReport %>% filter(type == 'factor' ) # factor  data
  
  # Remove columns that do not apply to this type of data -----------------------
  
  ## numeric data
  dataReport.numeric <- removeColsWithNA(dataReport.numeric)  %>%
    
    # Clean column names by removing numeric prefix, 
    rename_all(~str_replace(.,"numeric.","")) 
    
  ## factor  data
  dataReport.factor  <- removeColsWithNA(dataReport.factor ) %>%
  
    # Clean column names by removing factor  prefix
    rename_all(~str_replace(.,"factor.",""))  
  
  
  # Set up options for Display the reports
  options(skimr_strip_metadata = FALSE)
  options(digits=2)
  options(scipen=99)
  
  # Numeric report <- Get summary of data frame --------------------------------
  
    # data frame stats
    dfStats.num <- data.frame(Num_Numeric_Variables = ncol(df %>% select_if(is.numeric)),
                              Total_Observations    = nrow(df) )
    
    # Now see individual column statistics
    dfColStats.num <- dataReport.numeric %>% 
      dplyr::select(-type, -hist)
    
  
  # Factor report <- Get summary of data frame --------------------------------
  
    # Get summary of data frame
    dfStats.factor <- data.frame(Num_Factor_Variables = ncol(df %>% select_if(is.factor)),
                                 Total_Observations   = nrow(df) )
    
    # Now see individual column statistics
    dfColStats.factor <- dataReport.factor  %>% 
      dplyr::select(-type, -ordered) 
    
    
  # Return the data frames
  return(list('dfStats.num'       = dfStats.num,    
              'dfColStats.num'    = dfColStats.num,
              'dfStats.factor'    = dfStats.factor, 
              'dfColStats.factor' = dfColStats.factor))
}
```


## Numeric Data Quality Report

```{r}
# Get the factor and numeric reports
initialReport <- dataQualityReport(df.train.base)

# Numeric data frame stats
initialReport$dfStats.num %>% kable()

# Numeric column stats
initialReport$dfColStats.num %>%
  kable() %>% kable_styling(font_size=7, latex_options = 'HOLD_position') # numeric data

```


## Factor Data Quality Report

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# factor data frame stats
initialReport$dfStats.factor %>% kable()

# factor column stats
initialReport$dfColStats.factor %>%
  kable() %>% kable_styling(font_size=7, latex_options = 'HOLD_position') # numeric data
```

\newpage

## Exploratory Analysis and Visualizations

### Analysis 1: Exploring the Target Variable
```{r}
# names(df.train.base)

# Histogram of target variable and log(Target_variable + 1)
ggplot(df.train.base, aes(x=Target_Variable)) + 
  geom_histogram(aes(y=..density..), color="black", fill="deepskyblue") +
  geom_density(alpha=.6, fill="#FF6666") + 
  ggtitle("Histogram of Target_Variable")


ggplot(df.train.base, aes(x=log(Target_Variable + 1))) + 
  geom_histogram(aes(y=..density..), color="black", fill="deepskyblue") +
  geom_density(alpha=.6, fill="#FF6666") +
  ggtitle("Histogram of log(Target_Variable + 1)")

```
  
### Target Variable  
* The below exploratory analysis shows that the $Target\_Variable$ is skewed.  
* However, see that the $\log (Target\_Variable)$ is close to being normal, so 
we will need to transform this data.

\newpage  

```{r, echo=FALSE, results='hide'}
#Correlation between the numeric variables

# ggcorr(df.train.base, geom='blank', label=T, label_size=3, hjust=1,
#        size=3, layout.exp=2) +
#   geom_point(size = 4, aes(color = coefficient > 0, alpha = abs(coefficient) >= 0.5)) +
#   scale_alpha_manual(values = c("TRUE" = 0.25, "FALSE" = 0)) +
#   guides(color = F, alpha = F)
# 
# 
# cor(df.train.base.numeric) %>% knitr::kable()
# 
# # Correlation between factor  variables 
# aov.dat <- aov(Target_Variable ~ Delivery_Person_Id+Weather_Conditions+
#                  Road_Traffic_Density+Vehicle_Condition+Type_Of_Order+
#                  Type_Of_Vehicle+Multiple_Deliveries+Festival+City, 
#                data=df.train.base)
# summary(aov.dat)
```
  
### Analysis 2: Visualizations of interactions between Target variable and factor variables

```{r}

# Violin chart of Multiple_Deliveries vs Target_Variable
ggplot(df.train.base, aes(x=Multiple_Deliveries, y=Target_Variable, fill=Multiple_Deliveries)) +
  geom_violin() +
  coord_flip() + # This switch X and Y axis and allows to get the horizontal version
  theme(legend.position="none") +
  ggtitle("Violin chart: # of multiple deliveries vs target variable") +
  xlab("# Of multiple deliveries")+
  ylab('Time Taken to Deliver')+
  theme(text=element_text(size=10),
        plot.title.position = 'plot',
        plot.title = element_text(hjust = 0.5))


# Violin chart of City vs Target_Variable
ggplot(df.train.base, aes(x=City, y=Target_Variable, fill=City)) +
    geom_violin() +
    coord_flip() +
    theme(legend.position="none") +
    ggtitle("Violin chart: City vs target variable") +
    xlab("City") +
    ylab("Time Taken to Deliver")+
  theme(text=element_text(size=10),
        plot.title.position = 'plot',
        plot.title = element_text(hjust = 0.5))


# Boxplot of Vehicle_Condition vs Target_Variable. The Red dot is the mean of the group
# There was so much overlap with the violin plot, the boxplot is a better visual
ggplot(df.train.base, aes(x=Vehicle_Condition, y=Target_Variable, fill=Vehicle_Condition)) +
    geom_boxplot(alpha=0.5) +
    stat_summary(fun=mean, geom="point", shape=20, size=6, color="red", fill="red") +
    theme(legend.position="none") +
    scale_fill_brewer(palette="Set1")+
  ggtitle("Boxplot chart: Vehicle Condition vs target variable") +
    xlab("Vehicle Condition") +
    ylab("Time Taken to Deliver")+
  theme(text=element_text(size=10),
        plot.title.position = 'plot',
        plot.title = element_text(hjust = 0.5))
  

```

 The violin chart of "# of multiple deliveries vs target_variable"
 shows that the more deliveries you make, the more of the target_variable
 you make

 The Violin chart of "City vs target_variable" indicates that Semi-Urban areas 
 have the highest Target variable, and it is a slim distribution, meaning 
 it does not vary as much as the other distributions

 The Boxplot of "Vehicle_Condition vs Target_Variable" indicates that vehicle condition 
 0 and 3 have a higher target variable


\newpage

### Analysis 3: Visualizations of interactions between Target variable and numeric variables

```{r}

# Scatterplot of Delivery_Person_Ratings vs Target_Variable
# Multiple deliveries is being used to determine the color of a point to look for patterns
ggplot(df.train.base, aes(x=Delivery_Person_Ratings, y=Target_Variable, color=Multiple_Deliveries)) + 
  geom_point(alpha = 0.2, position=position_jitter(height=.5, width=.5)) +
  labs(title = "Scatterplot of Delivery Person's Ratings vs Target Variable",
       x="Ratings of Delivery Persons",
       y="Time Taken to Deliver",
       col="Multiple Deliveries")+
  theme(text=element_text(size=10),
        plot.title.position = 'plot',
        plot.title = element_text(hjust = 0.5))

# Scatterplot of Delivery_Person_Ratings vs Target_Variable
# City is being used to determine the color of a point to look for patterns
ggplot(df.train.base, aes(x=Delivery_Person_Ratings, y=Target_Variable, color=City)) + 
  geom_point(alpha = 0.2, position=position_jitter(height=.5, width=.5)) +
  labs(title = "Scatterplot of Delivery Person's Ratings vs Target Variable",
       x="Ratings of Delivery Persons",
       y="Time Taken to Deliver",
       col="City")+
  theme(text=element_text(size=10),
        plot.title.position = 'plot',
        plot.title = element_text(hjust = 0.5))


```

 This scatterplot is great at showing the relation ship of multiple deliveries
 and the increasing amount of the target_variable. Those with more deliveries,
 tend to have a higher target_variable. Another scatterplot was drawn with the
 city being used as the determination for color. This visual shows Semi-Urban
 having the highest amount of the target variable, followed by Urban and Metropolitian

\newpage

# Data Cleansing

## Missingingness
* To handle missingness, we will likely take the following approach for `numeric` and `factor` data:  
  - `Numeric`: Impute missing values using predictive mean matching with the `mice` package  
  - `Factor`: Leverage k-nearest neighbors to impute missing factor data. 
  This is likely possible because there is not a significant portion of the factor 
  data that is missing, so it should not be computationally extensive.    

## Outliers
1. We will prioritize limiting outliers of the target variable.  
2. We will also analyze each numeric independent variable to discover any outliers.
If there are few outliers, then we will likely omit that data. If outliers persist in
a large portion of the data, then we will limit the removal of outlying data.

## Skews

### Target Variable  
* The below exploratory analysis shows that the $Target\_Variable$ is skewed.  
* However, see that the $\log (Target\_Variable)$ is close to being normal, so 
we will not  need to transform this data.

### Other Numeric Predictors
If other numeric variables are highlight skewed (within the test and train data),
then we will likely use the `boxcox` function to normalize the test and training variables
associated.

## Factors
Since the factor data contains few unique values, we will not need to factor lump the data.
If there was many unique values in related variables, then we would factor lump to 
help fit the models more efficiently.

```{r}
# ==============================================================================
# NOTE THAT ALL WILL CLEANSING DONE IN THIS CODE CHUNK TO MAKE IT EASIER
# END GOAL IS TO CREATE A FUNCTION SO WE CAN EASILY APPLY TO TEST DATA
# TO REMOVE DUPLICATION
#
# Goal is to clean up:
#   - Missingingness
#   - Skews
#   - Outliers
#   - Factors
# ==============================================================================

# Copy data
df <- df.train.base # Full dataset

# If the data is training data, then convert to log(targetVariable)
isTrainingData = TRUE

if (isTrainingData) {
  df$Target_Variable <- log(df$Target_Variable)
}


df.numeric <- df.train.base.numeric # Numeric only columns
df.factor  <- df.train.base.factor  # Numeric only columns
df.imputed <- df                    # To store numeric imputation output




# Missingingness ---------------------------------------------------------------

### `Numeric`: Impute missing values using predictive mean matching with the `mice` package

# Note `see Imputation()` function to visualize the imputation
seeImputation <- function(df, df.meanInputed, 
                          imputationMethod) {
  
  # Min/Max ranges so actual and imputed histograms align
  yMin = quantile(df.meanInputed$y, 0.05)
  yMax = max(df.meanInputed$y)
  
  # Non Altered data -------------------------------------------------
  
  meanVal = mean(df$y, na.rm=T) # mean of the non altered data
  
  # Create the plot
  p1 <- df %>%
    ggplot(aes(x = y)) +
    
    # Histogram
    geom_histogram(color = 'grey65', fill = 'grey95') +
    
    # The mean value line
    geom_vline(xintercept = meanVal, color = 'tomato3') +
    
    # Text associated with mean value
    annotate("text", 
             label = "Mean Value", 
             x = meanVal, y = 100, 
             size = 5, colour = "tomato3" ) +
    
    # Labels
    labs(title = 'Data with Missing Values',
         y     = 'Frequency', 
         x     = '' ) +
    
    xlim(yMin, yMax) + # min and max range of x axis (for equal comparison)
    theme_minimal() # Theme
  
  
  # Imputed data -------------------------------------------------
  meanValImpute = mean(df.meanInputed$y, na.rm=T)
  
  # Create the plot
  p2 <- df.meanInputed %>%
    ggplot(aes(x = y)) +
    
    # Histogram
    geom_histogram(color = 'grey65', fill = 'grey95') +
    
    # The mean value line
    geom_vline(xintercept = meanVal, color = 'tomato3') +
    
    # Text associated with mean value
    annotate("text", 
             label = "Mean Value", 
             x = meanValImpute, y = 100, 
             size = 5, colour = "tomato3" ) +
    
    # Labels
    labs(title = 'Data without Missing Values',
             subtitle = 'Using PMM',
             y = 'Frequency', 
             x = imputationMethod) +
    
    xlim(yMin, yMax) + # min and max range of x axis (for equal comparison)
    theme_minimal() # Theme
  
  # Variation scatter ----------------------------------------------------------
  
  p3 <- df.meanInputed %>% ggplot(aes(x=rexp(length(y)), y=y, color=is.na(df$y))) + 
    
    # Add points
    geom_point(alpha = 0.5) +
    
    # Colors, limits, labels, and themes
    scale_color_manual(values = c('grey80', 'tomato3'),
                       labels = c('Actuals', 'Imputed') ) +
    ylim(0, quantile(df.meanInputed$y, 0.99)) + # lower 99% of dist
    labs(title   = 'Variation of Actuals vs. Imputed Data',
         x       = 'x', 
         y       = imputationMethod,
         caption =paste0('\nUsing housing.csv data',
                         '\nOnly showing lower 99% of distribution for viewing') 
         ) +
    theme_minimal() + theme(legend.position = 'bottom',
                            legend.title    = element_blank())
  
  
  # Combine the plots for the final returned output
  combinedPlots <- plot_grid(p1, p2, p3, 
                             ncol = 1, label_size = 12,
                             rel_heights = c(1, 1.1, 1.75))
  return(combinedPlots)
}

# Create function to impute via `PMM`
imputeWithPMM <- function(colWithMissingData) {
  
  # Using the mice package
  suppressMessages(library(mice))
  
  # Discover the missing rows
  isMissing <- is.na(colWithMissingData) 
  
  # Create data frame to pass to PMM imputation function from mic package
  df <- data.frame(x       = rexp(length(colWithMissingData)), # meaningless x to help show variation 
                   y       = colWithMissingData, 
                   missing = isMissing)
  
  # imputation by PMM
  df[isMissing, "y"] <- mice.impute.pmm( df$y, 
                                        !df$missing, 
                                         df$x)
  
  return(df$y)
}


# Which columns has Na's?
colNamesWithNulls.num <- colnames(df.numeric[ , colSums(is.na(df.numeric)) != 0])
colNamesWithNulls.num

numberOfColsWithNulls = length(colNamesWithNulls.num)

# For each of the numeric columns with null values
for (colWithNullsNum in 1:numberOfColsWithNulls) {
  
  # The name of the column with null values
  nameOfThisColumn <- colNamesWithNulls.num[colWithNullsNum]
  
  # Get the actual data of the column with nulls
  colWithNulls <- df[, nameOfThisColumn]
  
  # Impute the missing values with PMM
  imputedValues.num <- imputeWithPMM(colWithNulls)
  
  # Now store the data in the imputed data frame
  df.imputed[, nameOfThisColumn] <- imputedValues.num
  
  # Save a visualization of the imputation
  pmmVisual <- seeImputation(data.frame(y = colWithNulls),
                             data.frame(y = imputedValues.num),
                             nameOfThisColumn )
  
  # Uncomment if you want to visual the PMM imputation
  # fileToSave = paste0('OutputPMM/Imputation_With_PMM_', nameOfThisColumn, '.pdf')
  # print(paste0('For imputation results of ', nameOfThisColumn, ', see ', fileToSave))
  # dir.create("OutputPMM/")
  # ggsave(pmmVisual, filename = fileToSave,
  #        height = 11, width = 8.5)
}

# colnames(df.imputed[ , colSums(is.na(df.imputed)) != 0]) # Check to see if it worked for numeric fields


### `Factor`: Leverage k-nearest neighbors to impute missing factor data. ------

# Get the factor data with missing columns
colNamesWithNulls.factor <- colnames(df.factor[ , colSums(is.na(df.factor)) != 0])
colNamesWithNulls.factor

NUM_NEIGHBORS = 5 # number of neighbors

# Impute missing factor data for columns with missing data
imputedValues.factor <- kNN( df[, colNamesWithNulls.factor], k=NUM_NEIGHBORS )

# Remove the columns indicating if factor data was imputed
imputedValues.factor <- imputedValues.factor %>% dplyr::select(-ends_with('imp'))

# Now store the data in the imputed data frame
df.imputed[, colNamesWithNulls.factor] <- imputedValues.factor

# colnames(df.imputed[ , colSums(is.na(df.imputed)) != 0]) # Check to see if it worked


# Skews ------------------------------------------------------------------------

### If other numeric variables are highlight skewed (within the test and train data),
### then we will use the `boxcox` function to normalize the test and training variables
### associated. Boxcox cannot handle negative values

df.imputedNormalized <- df.imputed # Copy

### Function to Convert Skewed Data to Normally Distributed Vector
normalizeDist <- function(aVector) {
  
  # Get the optimal lambda. Used later for converting to normal distribution
  normLambda = boxcox(aVector, optimize = TRUE)$lambda
  
  # Now convert vector to normal distribution, using the optimal lambda
  normalizedVector <- (aVector ** normLambda - 1) / normLambda
  
  return(normalizedVector)
}

# Uncomment to see that these are not normal
# Did not mess with the lat/long values
# hist(df.imputedNormalized$Time_Ordered )
# hist(df.imputedNormalized$Time_Order_Picked )
# hist(df.imputedNormalized$Target_Variable)

# Mutate 
df.imputedNormalized <- df.imputedNormalized %>%
  mutate_at(vars(Time_Ordered, Time_Order_Picked, Target_Variable), 
            normalizeDist)

# Outliers ---------------------------------------------------------------------
## 1. We will prioritize limiting outliers of the target variable.  
## 2. We will also analyze each numeric independent variable to discover any outliers.
## If there are few outliers, then we will likely omit that data. If outliers persist in
## a large portion of the data, then we will limit the removal of outlying data.

# Copy data
df.impNormNoOutliers <- df.imputedNormalized

k_outliers  = 50  # remove up to 50 outliers per column
numOutliers = c() # to store the number of outliers per column\
outlyingColumns = c() # to store columns with outliers
theColNames <- colnames(df.impNormNoOutliers) # column names

# For each numeric column remove up to 50 outliers
for (colNum in 1:ncol(df.impNormNoOutliers)) {
  theCol <- df.impNormNoOutliers[, colNum]
  nrowBefore = length(theCol)
  colName <- theColNames[colNum]
  
  # Only consider numeric
  if (is.numeric(theCol)) {
        
    # Identify the outliers in the column
    # Source: https://www.geeksforgeeks.org/remove-outliers-from-data-set-in-r/
    columnOutliers <- boxplot.stats(df.impNormNoOutliers[, colNum])$out
    numOutliersInCol <- length(columnOutliers)
    
    # outlying Columns 
    numOutliers <- c(numOutliers, length(columnOutliers))
    
    # Now remove k outliers from the column
    if (length(columnOutliers) < k_outliers) {
      
      df.impNormNoOutliers  <- df.impNormNoOutliers %>%
        
        # If this syntax looks weird, it is just referencing a column in the 
        # data set using dplyr piping. See below for more info:
        # https://stackoverflow.com/questions/48062213/dplyr-using-column-names-as-function-arguments
        # https://stackoverflow.com/questions/72673381/column-names-as-variables-in-dplyr-select-v-filter
        filter( !( get({{colName}}) %in% columnOutliers ) )
    }
  }
}
paste('Num rows before:', nrow(df.imputedNormalized))
paste('Num rows after:',  nrow(df.impNormNoOutliers))


# Factors ----------------------------------------------------------------------
## Since the factor data contains few unique values, we will not need to factor lump the data.
## If there was many unique values in related variables, then we would factor lump to 
## help fit the models more efficiently.


# Create a cleaned dataset for modeling
# Does not include any identifiers
df.clean <- df.impNormNoOutliers %>%
  dplyr::select(-c(Id,
                   Delivery_Person_Id,
                   Name))

```


\newpage 

# Modeling

## Model 1: OLS Test
```{r}
# Fit Initial model
fit.ols <- lm(Target_Variable ~ Delivery_Person_Age + Delivery_Person_Ratings + 
                Restaurant_Latitude + Weather_Conditions + Road_Traffic_Density + 
                Vehicle_Condition + Type_Of_Vehicle + Multiple_Deliveries + 
                Festival + City + Time_Order_Picked,
              data = df.clean)

# See fit
# summary(fit.ols)


# Use stepAIC to discover better model
# fit.ols.stepAIC <- stepAIC(fit.ols, direction = "both")
# summary(fit.ols.stepAIC)

# Output model
# Target_Variable ~ Delivery_Person_Age + Delivery_Person_Ratings + 
#     Restaurant_Latitude + Weather_Conditions + Road_Traffic_Density + 
#     Vehicle_Condition + Type_Of_Vehicle + Multiple_Deliveries + 
#     Festival + City + Time_Order_Picked
```


```{r}
# OLS Diagnostics

# Get the RMSE and R Squared of the model
ols.rmse    <- rmse(actual=df.clean$Target_Variable, predicted=fit.ols$fitted.values)
ols.summary <- summary(fit.ols)

# Key diagnostics
keyDiagnostics.ols <- data.frame(Model    = 'OLS',
                                 Notes    = 'lm',
                                 Hyperparameters = 'N/A',
                                 RMSE     = ols.rmse,
                                 Rsquared = ols.summary$adj.r.squared)

# Show output
keyDiagnostics.ols %>% 
  knitr::kable()
```


## Model 2: MARS Test
```{r}
# Model tuning controls
ctrl <- trainControl(method  = "repeatedcv", 
                     number  = 5, # 5 fold cross validation
                     repeats = 1  # 1 repeats
                     )

# Fit the model
fit.mars <- train(data = df.clean, 
                  Target_Variable ~ Delivery_Person_Age + Delivery_Person_Ratings + 
                    Restaurant_Latitude + Weather_Conditions + Road_Traffic_Density + 
                    Vehicle_Condition + Type_Of_Vehicle + Multiple_Deliveries + 
                    Festival + City + Time_Order_Picked,
                  method     = "earth",             # Earth is for MARS models
                  tuneLength = 9,                   # 9 values of the cost function
                  preProc    = c("center","scale"), # Center and scale data
                  trControl  = ctrl 
                 )

# summary(fit.mars)
```


```{r}
# MARS Diagnostics

# Get the RMSE and R Squared of the model
hyperparameters.mars = list('degree' = fit.mars[["bestTune"]][["degree"]],
                            'nprune' = fit.mars[["bestTune"]][["nprune"]])

keyDiagnostics.mars <- data.frame(Model   = 'MARS',
                                  Notes    = 'caret and earth',
                                  Hyperparameters = paste('Degree =', hyperparameters.mars$degree, ',',
                                                          'nprune =', hyperparameters.mars$nprune)
                                  )

keyDiagnostics.mars <- cbind(keyDiagnostics.mars,
                            fit.mars$results %>% 
                              filter(degree == hyperparameters.mars$degree,
                                     nprune == hyperparameters.mars$nprune) %>%
                              dplyr::select(RMSE, Rsquared)
                      )

# Show output
keyDiagnostics.mars %>% kable()
```


## Model 3: Elastic Net Model  
  
```{r}
# Fit the model
fit.elasticnet <- train(data = df.clean, 
                  Target_Variable ~ Delivery_Person_Age + Delivery_Person_Ratings + 
                    Restaurant_Latitude + Weather_Conditions + Road_Traffic_Density + 
                    Vehicle_Condition + Type_Of_Vehicle + Multiple_Deliveries + 
                    Festival + City + Time_Order_Picked,
                  method     = "glmnet",            # Elastic net
                  tuneLength = 10,                   # 9 values of the cost function
                  preProc    = c("center","scale"), # Center and scale data
                  trControl  = ctrl 
                 )

# summary(fit.elasticnet)
```


```{r}
# Function to get the best hypertuned parameters
get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}
result.elasticnet <- get_best_result(fit.elasticnet)

# Gather key diagnostics for summary table
# Get the RMSE and R Squared of the model
hyperparameters.elasticnet = list('Alpha'  = result.elasticnet$alpha,
                                  'Lambda' = result.elasticnet$lambda)


keyDiagnostics.elasticnet <- data.frame(Model    = 'Elastic Net',
                                        Notes    = 'caret and elasticnet',
                                        Hyperparameters = paste('Alpha =',
                                                                hyperparameters.elasticnet$Alpha, ',',
                                                                'Lambda =',
                                                                hyperparameters.elasticnet$Lambda),
                                        RMSE     = result.elasticnet$RMSE,
                                        Rsquared = result.elasticnet$Rsquared
                                        )

# Show output
keyDiagnostics.elasticnet %>% knitr::kable()
```
  
  
### Model 4: PCR Model  

```{r}
# Fit the model
fit.pcr <- mvr(data = df.clean, 
                  Target_Variable ~ Delivery_Person_Age + Delivery_Person_Ratings + 
                    Restaurant_Latitude + Weather_Conditions + Road_Traffic_Density + 
                    Vehicle_Condition + Type_Of_Vehicle + Multiple_Deliveries + 
                    Festival + City + Time_Order_Picked,
               center = TRUE,
               scale  = TRUE, 
               validation = "CV" 
                 )

# summary(fit.pcr)
```


```{r}
# See the summary output
summary(fit.pcr)

validationplot(fit.pcr)
validationplot(fit.pcr, val.type = 'R2')
``` 

```{r}
# Gather key diagnostics for summary table
# Get the RMSE and R Squared of the model
# Key diagnostics for PCR final summary table
RMSE.pcr <- RMSEP(fit.pcr, ncomp=15)
R2.pcr <- R2(fit.pcr, ncomp = 1:15)

# Get the RMSE and R Squared of the model
keyDiagnostics.pcr <- data.frame(Model    = 'PCR',
                                 Notes    = 'pcr',
                                 Hyperparameters = paste('ncomp = ', 15),
                                 RMSE     = min(RMSE.pcr$val),
                                 Rsquared = max(R2.pcr$val) )

# Show output
keyDiagnostics.pcr %>% 
  knitr::kable()
```

### Model 5: Decision Trees  
  
```{r}
#using data without any pre-processing
fit.cart <- train(data = df.clean, 
                  Target_Variable ~ Delivery_Person_Age + Delivery_Person_Ratings +
                    Restaurant_Latitude + Weather_Conditions + Road_Traffic_Density +
                    Vehicle_Condition + Type_Of_Vehicle + Multiple_Deliveries + 
                    Festival + City + Time_Order_Picked,
                  method="rpart",
                  tuneLength = 10,                   # 9 values of the cost function
                  #preProc    = c("center","scale"),
                  trControl=ctrl) 

# Get the RMSE and R Squared of the model
hyperparameters.cart = list('cp' = fit.cart[["bestTune"]][["cp"]])


keyDiagnostics.cart <- data.frame(Model    = 'CART',
                                        Notes    = 'rpart',
                                        Hyperparameters = paste('cp =',
                                                                hyperparameters.cart$cp),
                                        RMSE     = fit.cart$results[1,'RMSE'],
                                        Rsquared = fit.cart$results[1, 'Rsquared']
                                        )

# Show output
keyDiagnostics.cart %>% knitr::kable()

```  
  
### Model 6: Random Forest

```{r}
fit.rf <- train(data = df.clean, 
                  Target_Variable ~ Delivery_Person_Age + Delivery_Person_Ratings +
                    Restaurant_Latitude + Weather_Conditions + Road_Traffic_Density +
                    Vehicle_Condition + Type_Of_Vehicle + Multiple_Deliveries + 
                    Festival + City + Time_Order_Picked, 
                method="rf", 
                tuneLength = 10,                   # 9 values of the cost function
                #preProc    = c("center","scale"),
                trControl=ctrl,
                allowParallel = TRUE) 

# Get the RMSE and R Squared of the model
hyperparameters.rf = list('mtry' = fit.rf[["bestTune"]][["mtry"]])


keyDiagnostics.rf <- data.frame(Model    = 'Random Forest',
                                        Notes    = 'rf',
                                        Hyperparameters = paste('mtry =',
                                                                hyperparameters.rf$mtry),
                                        RMSE     = fit.rf$results[3,'RMSE'],
                                        Rsquared = fit.cart$results[3, 'Rsquared'])

# Show output
keyDiagnostics.rf %>% knitr::kable()

```
  

### Model 7: Gradient Boost Trees

```{r}
## gradient boosting
fit.grboost <- train(data = df.clean, 
                     Target_Variable ~ Delivery_Person_Age + Delivery_Person_Ratings +
                       Restaurant_Latitude + Weather_Conditions + Road_Traffic_Density +
                       Vehicle_Condition + Type_Of_Vehicle + Multiple_Deliveries + 
                       Festival + City + Time_Order_Picked, 
                     method = "xgbTree",
                     trControl = ctrl
  )
result.grboost = fit.grboost$results %>% arrange(fit.grboost$results[,'RMSE'])
hyperparameters.grboost = list('max_depth' = fit.grboost[["bestTune"]][["max_depth"]],
                            'eta' = fit.grboost[["bestTune"]][["eta"]],
                            'nrounds' = fit.grboost[["bestTune"]][["nrounds"]])
# Key diagnostics
keyDiagnostics.grboost <- data.frame(Model    = 'Gradient boost',
                                 Notes    = 'xgbTree',
                                 Hyperparameters = paste('max_depth =', hyperparameters.grboost$max_depth, ',',
                                                          'eta =', hyperparameters.grboost$eta, ',',
                                                   'nrounds=', hyperparameters.grboost$nrounds),
                                 RMSE = result.grboost[1,'RMSE'],
                                 Rsquared = result.grboost[1,'Rsquared'])


# Show output
keyDiagnostics.grboost %>% 
  knitr::kable()

```
  
  
```{r}
# Add the key diagnostics here
rbind(
  keyDiagnostics.ols,
  keyDiagnostics.mars,
  keyDiagnostics.elasticnet,
  keyDiagnostics.pcr,
  keyDiagnostics.cart,
  keyDiagnostics.rf,
  keyDiagnostics.grboost
) %>%
  
  # Round to 4 digits across numeric data
  mutate_if(is.numeric, round, digits = 4) %>%
  
  # Spit out kable table
  kable()
```  
  
\newpage

# Apply to Test Data  

* Need to clean test data like we did in the train  

* Note all comments for the main model apply here  

* Then apply the models to this dataset

* Outputs a CSV with predicted customer log revenue

* For general data preparation, please see conceptual steps below. See `.rmd` file for detailed code.  

  
```{r}

# Copy data
df <- df.test.base # Full dataset

# If the data is training data, then convert to log(targetVariable)
isTrainingData = FALSE

if (isTrainingData) {
  df$Target_Variable <- log(df$Target_Variable)
}


df.numeric <- df.test.base.numeric # Numeric only columns
df.factor  <- df.test.base.factor  # Numeric only columns
df.imputed <- df                    # To store numeric imputation output




# Missingingness ---------------------------------------------------------------

### `Numeric`: Impute missing values using predictive mean matching with the `mice` package

# Note `see Imputation()` function to visualize the imputation
seeImputation <- function(df, df.meanInputed, 
                          imputationMethod) {
  
  # Min/Max ranges so actual and imputed histograms align
  yMin = quantile(df.meanInputed$y, 0.05)
  yMax = max(df.meanInputed$y)
  
  # Non Altered data -------------------------------------------------
  
  meanVal = mean(df$y, na.rm=T) # mean of the non altered data
  
  # Create the plot
  p1 <- df %>%
    ggplot(aes(x = y)) +
    
    # Histogram
    geom_histogram(color = 'grey65', fill = 'grey95') +
    
    # The mean value line
    geom_vline(xintercept = meanVal, color = 'tomato3') +
    
    # Text associated with mean value
    annotate("text", 
             label = "Mean Value", 
             x = meanVal, y = 100, 
             size = 5, colour = "tomato3" ) +
    
    # Labels
    labs(title = 'Data with Missing Values',
         y     = 'Frequency', 
         x     = '' ) +
    
    xlim(yMin, yMax) + # min and max range of x axis (for equal comparison)
    theme_minimal() # Theme
  
  
  # Imputed data -------------------------------------------------
  meanValImpute = mean(df.meanInputed$y, na.rm=T)
  
  # Create the plot
  p2 <- df.meanInputed %>%
    ggplot(aes(x = y)) +
    
    # Histogram
    geom_histogram(color = 'grey65', fill = 'grey95') +
    
    # The mean value line
    geom_vline(xintercept = meanVal, color = 'tomato3') +
    
    # Text associated with mean value
    annotate("text", 
             label = "Mean Value", 
             x = meanValImpute, y = 100, 
             size = 5, colour = "tomato3" ) +
    
    # Labels
    labs(title = 'Data without Missing Values',
             subtitle = 'Using PMM',
             y = 'Frequency', 
             x = imputationMethod) +
    
    xlim(yMin, yMax) + # min and max range of x axis (for equal comparison)
    theme_minimal() # Theme
  
  # Variation scatter ----------------------------------------------------------
  
  p3 <- df.meanInputed %>% ggplot(aes(x=rexp(length(y)), y=y, color=is.na(df$y))) + 
    
    # Add points
    geom_point(alpha = 0.5) +
    
    # Colors, limits, labels, and themes
    scale_color_manual(values = c('grey80', 'tomato3'),
                       labels = c('Actuals', 'Imputed') ) +
    ylim(0, quantile(df.meanInputed$y, 0.99)) + # lower 99% of dist
    labs(title   = 'Variation of Actuals vs. Imputed Data',
         x       = 'x', 
         y       = imputationMethod,
         caption =paste0('\nUsing housing.csv data',
                         '\nOnly showing lower 99% of distribution for viewing') 
         ) +
    theme_minimal() + theme(legend.position = 'bottom',
                            legend.title    = element_blank())
  
  
  # Combine the plots for the final returned output
  combinedPlots <- plot_grid(p1, p2, p3, 
                             ncol = 1, label_size = 12,
                             rel_heights = c(1, 1.1, 1.75))
  return(combinedPlots)
}

# Create function to impute via `PMM`
imputeWithPMM <- function(colWithMissingData) {
  
  # Using the mice package
  suppressMessages(library(mice))
  
  # Discover the missing rows
  isMissing <- is.na(colWithMissingData) 
  
  # Create data frame to pass to PMM imputation function from mic package
  df <- data.frame(x       = rexp(length(colWithMissingData)), # meaningless x to help show variation 
                   y       = colWithMissingData, 
                   missing = isMissing)
  
  # imputation by PMM
  df[isMissing, "y"] <- mice.impute.pmm( df$y, 
                                        !df$missing, 
                                         df$x)
  
  return(df$y)
}


# Which columns has Na's?
colNamesWithNulls.num <- colnames(df.numeric[ , colSums(is.na(df.numeric)) != 0])
colNamesWithNulls.num

numberOfColsWithNulls = length(colNamesWithNulls.num)

# For each of the numeric columns with null values
for (colWithNullsNum in 1:numberOfColsWithNulls) {
  
  # The name of the column with null values
  nameOfThisColumn <- colNamesWithNulls.num[colWithNullsNum]
  
  # Get the actual data of the column with nulls
  colWithNulls <- df[, nameOfThisColumn]
  
  # Impute the missing values with PMM
  imputedValues.num <- imputeWithPMM(colWithNulls)
  
  # Now store the data in the imputed data frame
  df.imputed[, nameOfThisColumn] <- imputedValues.num
  
  # Save a visualization of the imputation
  pmmVisual <- seeImputation(data.frame(y = colWithNulls),
                             data.frame(y = imputedValues.num),
                             nameOfThisColumn )
  
  # Uncomment if you want to visual the PMM imputation
  # fileToSave = paste0('OutputPMM/Imputation_With_PMM_', nameOfThisColumn, '.pdf')
  # print(paste0('For imputation results of ', nameOfThisColumn, ', see ', fileToSave))
  # dir.create("OutputPMM/")
  # ggsave(pmmVisual, filename = fileToSave,
  #        height = 11, width = 8.5)
}

# colnames(df.imputed[ , colSums(is.na(df.imputed)) != 0]) # Check to see if it worked for numeric fields


### `Factor`: Leverage k-nearest neighbors to impute missing factor data. ------

# Get the factor data with missing columns
colNamesWithNulls.factor <- colnames(df.factor[ , colSums(is.na(df.factor)) != 0])
colNamesWithNulls.factor

NUM_NEIGHBORS = 5 # number of neighbors

# Impute missing factor data for columns with missing data
imputedValues.factor <- kNN( df[, colNamesWithNulls.factor], k=NUM_NEIGHBORS )

# Remove the columns indicating if factor data was imputed
imputedValues.factor <- imputedValues.factor %>% dplyr::select(-ends_with('imp'))

# Now store the data in the imputed data frame
df.imputed[, colNamesWithNulls.factor] <- imputedValues.factor

# colnames(df.imputed[ , colSums(is.na(df.imputed)) != 0]) # Check to see if it worked


# Skews ------------------------------------------------------------------------

### If other numeric variables are highlight skewed (within the test and train data),
### then we will use the `boxcox` function to normalize the test and training variables
### associated. Boxcox cannot handle negative values

df.imputedNormalized <- df.imputed # Copy

### Function to Convert Skewed Data to Normally Distributed Vector
normalizeDist <- function(aVector) {
  
  # Get the optimal lambda. Used later for converting to normal distribution
  normLambda = boxcox(aVector, optimize = TRUE)$lambda
  
  # Now convert vector to normal distribution, using the optimal lambda
  normalizedVector <- (aVector ** normLambda - 1) / normLambda
  
  return(normalizedVector)
}

# Uncomment to see that these are not normal
# Did not mess with the lat/long values
# hist(df.imputedNormalized$Time_Ordered )
# hist(df.imputedNormalized$Time_Order_Picked )
# hist(df.imputedNormalized$Target_Variable)

# Mutate 
df.imputedNormalized <- df.imputedNormalized %>%
  mutate_at(vars(Time_Ordered, Time_Order_Picked), 
            normalizeDist)

# Outliers ---------------------------------------------------------------------
## 1. We will prioritize limiting outliers of the target variable.  
## 2. We will also analyze each numeric independent variable to discover any outliers.
## If there are few outliers, then we will likely omit that data. If outliers persist in
## a large portion of the data, then we will limit the removal of outlying data.

# Copy data
df.impNormNoOutliers <- df.imputedNormalized

k_outliers  = 50  # remove up to 50 outliers per column
numOutliers = c() # to store the number of outliers per column\
outlyingColumns = c() # to store columns with outliers
theColNames <- colnames(df.impNormNoOutliers) # column names

# For each numeric column remove up to 50 outliers
for (colNum in 1:ncol(df.impNormNoOutliers)) {
  theCol <- df.impNormNoOutliers[, colNum]
  nrowBefore = length(theCol)
  colName <- theColNames[colNum]
  
  # Only consider numeric
  if (is.numeric(theCol)) {
        
    # Identify the outliers in the column
    # Source: https://www.geeksforgeeks.org/remove-outliers-from-data-set-in-r/
    columnOutliers <- boxplot.stats(df.impNormNoOutliers[, colNum])$out
    numOutliersInCol <- length(columnOutliers)
    
    # outlying Columns 
    numOutliers <- c(numOutliers, length(columnOutliers))
    
    # Now remove k outliers from the column
    if (length(columnOutliers) < k_outliers) {
      
      df.impNormNoOutliers  <- df.impNormNoOutliers %>%
        
        # If this syntax looks weird, it is just referencing a column in the 
        # data set using dplyr piping. See below for more info:
        # https://stackoverflow.com/questions/48062213/dplyr-using-column-names-as-function-arguments
        # https://stackoverflow.com/questions/72673381/column-names-as-variables-in-dplyr-select-v-filter
        filter( !( get({{colName}}) %in% columnOutliers ) )
    }
  }
}
paste('Num rows before:', nrow(df.imputedNormalized))
paste('Num rows after:',  nrow(df.impNormNoOutliers))


# Factors ----------------------------------------------------------------------
## Since the factor data contains few unique values, we will not need to factor lump the data.
## If there was many unique values in related variables, then we would factor lump to 
## help fit the models more efficiently.


# Create a cleaned dataset for modeling
# Does not include any identifiers
df.clean.test.Id <- df.impNormNoOutliers
df.clean.test <- df.impNormNoOutliers %>%
  dplyr::select(-c(Id,
                   Delivery_Person_Id,
                   Name))
```  
  
```{r}
## Predict the customer data using the Random Forest model
predictions <- predict(fit.rf,df.clean.test)
```