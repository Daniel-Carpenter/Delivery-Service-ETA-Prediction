---
title:    "ISE 5103 Intelligent Data Analytics"
subtitle: "Final Project"
author:   "Daniel Carpenter, Sonaxy Mohanty, & Zachary Knepp"
date:     "December 2022"
output: 
  pdf_document:
    toc: true
    number_sections: true
    toc_depth: 2
    highlight: arrow
    latex_engine: xelatex
  # github_document:
  #   toc: yes
  #   toc_depth: 2
urlcolor: blue
cache: true
---

```{r setup, include=FALSE}            
                                       # By default...
knitr::opts_chunk$set(echo    = FALSE, # do not show code
                      message = FALSE, # Do not show messages
                      warning = FALSE, # Do not show warning messages
                      
                      # Figures
                      fig.width  = 5,
                      fig.height = 3, 
                      fig.align  = 'center'
                      )
```


```{r error=FALSE}
# Packages --------

# Data Wrangling
library(tidyverse)
library(skimr)
library(lubridate) # dates

# Modeling
library(MASS)
library(caret) # Modeling variants like SVM
library(earth) # Modeling with Mars
library(pls)   # Modeling with PLS
library(glmnet) # Modeling with LASSO

# Aesthetics
library(knitr)
library(cowplot)  # multiple ggplots on one plot with plot_grid()
library(scales)
library(kableExtra)
library(inspectdf)
library(GGally) # Pairs plots

#Hold-out Validation
library(caTools)

#Data Correlation
library(GGally)
library(regclass)

#RMSE Calculation
library(Metrics)

#p-value for OLS model
library(broom)

#ncvTest
library(car)

# For visualizations
library(hrbrthemes)
library(viridis)
```

\newpage

# General Data Prep
> For general data preparation, please see conceptual steps below. See `.rmd` file for detailed code.

## Creating the CSV Dataset
* Note that the original training and test data [found here](https://www.kaggle.com/datasets/radadiyamohit/time-taken-by-delivery-person)
contains two zipped files totaling around 55,000 `.txt` files  
* In order to convert this data into a usable format, we created a function that:  
  - Reads all `txt` contained within a specified folder  
  - Cleans whitespace, variable naming conventions, and converts `Time_Ordered` 
  and `Time_Ordered_Picked` from HH:MM string time.  
  - All variables are cast to their correct data types  
  - Finally, the data is exported to a single CSV.  
  - This function is applied to to the training and test data  
  - [This R file containing the function is located here](https://github.com/Daniel-Carpenter/IDA-Final-Project/blob/main/2%20-%20Initial%20Data%20Analysis/ReadAndCleanTxtFiles.R)  
* Note that the function is not run within this file due to the time required to run the code. Since there are so many files, it takes a large amount of time.  


## Read Training and Test Data
* Read [training](https://raw.githubusercontent.com/Daniel-Carpenter/IDA-Final-Project/main/2%20-%20Initial%20Data%20Analysis/TrainingData.csv) and [test](https://raw.githubusercontent.com/Daniel-Carpenter/IDA-Final-Project/main/2%20-%20Initial%20Data%20Analysis/TestData.csv) data CSV files from GitHub  
* Clean data to ensure each read variable has the correct data type (factor, numeric, Date, etc.)  

```{r, cache=TRUE}
# Read in Data -----------------------------------------------------------------

# Function to read from Git and convert variable types
readCleanFromGit <- function(gitDirectoryURL, csvName, isTrainingData) {
  
  ## GitHub URL's
  csvFullURL <- paste0(gitDirectoryURL, csvName)
  
  ## Read the data from GitHub
  df <- read.csv(csvFullURL, stringsAsFactors = TRUE) # Training data
  
  # Ensure Type Conversion -----------------------------------------------------
  
  ## Convert all character data to factor
  df.goodTypes <- df %>% 
    
    ### Ensure boolean variables are numeric
    mutate_at(vars(
      Delivery_Person_Age,
      Delivery_Person_Ratings,
      Restaurant_Latitude,
      Restaurant_Longitude,
      Delivery_Location_Latitude,
      Delivery_Location_Longitude,
      Restaurant_Longitude,
      Time_Ordered,
      Time_Order_Picked
      ), as.numeric) %>%
    
    ### Make sure dates are dates
    mutate_at(vars(Order_Date), as.Date) %>%
  
    ### Ensure factor are factors
    mutate_at(vars(
      Id,
      Delivery_Person_Id,
      Weather_Conditions,
      Road_Traffic_Density,
      Vehicle_Condition,
      Type_Of_Order,
      Type_Of_Vehicle,
      Multiple_Deliveries,
      City,
      Name
    ), as.factor) %>%
    
    mutate(Festival = as.factor(if_else(is.na(Festival), 0, 1)) )
      
  # If training data then ensure target variable is good.
  if (isTrainingData) {
    df.goodTypes <- df.goodTypes %>%
      mutate(Target_Variable = as.numeric(Target_Variable) )
  }
  
  # Return csv with type conversion
  return(df.goodTypes)
} 


# Actually read in the data and clean from Git ---------------------------------
gitDirectory <- 'https://raw.githubusercontent.com/Daniel-Carpenter/IDA-Final-Project/main/2%20-%20Initial%20Data%20Analysis/'


## Training data
df.train.base <- readCleanFromGit(gitDirectoryURL = gitDirectory, 
                                 csvName         = 'TrainingData.csv',
                                 isTrainingData  = TRUE)

## Test data
df.test.base  <- readCleanFromGit(gitDirectoryURL = gitDirectory, 
                                  csvName         = 'TestData.csv',
                                  isTrainingData  = FALSE)

# Test to see if type conversion worked
# lapply(df.train.base, class)
# lapply(df.test.base, class)
```


## Create `numeric` and `factor` *base* `data frames`
```{r}
# Function to separate into factor and numeric data ----------------------------
separateFactorAndNumeric <- function(df, functionType) {
  
  # Numeric Data frame
  df.typeSubset<- df %>%
  
    # selecting all the ______ data, e.g. is.numeric
    dplyr::select_if(functionType) %>%
  
    # converting the data frame to tibble
    as_tibble()
  
  return(df.typeSubset)
}


# Create the factor and numeric data -------------------------------------------

## Training data
df.train.base.numeric <- separateFactorAndNumeric(df.train.base, is.numeric) # numeric
df.train.base.factor  <- separateFactorAndNumeric(df.train.base, is.factor)  # factor

## Test Data
df.test.base.numeric  <- separateFactorAndNumeric(df.train.base, is.numeric) # numeric
df.test.base.factor   <- separateFactorAndNumeric(df.train.base, is.factor)  # factor
```

\newpage

# Data Understanding
> Create a data quality report of `numeric` and `factor` data  
> Created function called `dataQualityReport()` to create factor and numeric QA report

```{r}
# Function for data report
dataQualityReport <- function(df) {
  
  # Function to remove any columns with NA
  removeColsWithNA <- function(df) {
    return( df[ , colSums(is.na(df)) == 0] )
  }
  
  # Create Comprehensive data report using skimr package
  # This is done a bit piece-wise because PDF latex does not like the skimr package
  # Very much. So Instead of printing `skim(df)`, I have to pull the contents manually
  # Unfortunately. This is not an issue with html typically.
  dataReport <- skim(df) %>%
    rename_all(~str_replace(.,"skim_","")) %>%
    arrange(type, desc(complete_rate) ) # sort data 
  
  # Filter to the class types
  dataReport.numeric <- dataReport %>% filter(type == 'numeric') # numeric data
  dataReport.factor  <- dataReport %>% filter(type == 'factor' ) # factor  data
  
  # Remove columns that do not apply to this type of data -----------------------
  
  ## numeric data
  dataReport.numeric <- removeColsWithNA(dataReport.numeric)  %>%
    
    # Clean column names by removing numeric prefix, 
    rename_all(~str_replace(.,"numeric.","")) 
    
  ## factor  data
  dataReport.factor  <- removeColsWithNA(dataReport.factor ) %>%
  
    # Clean column names by removing factor  prefix
    rename_all(~str_replace(.,"factor.",""))  
  
  
  # Set up options for Display the reports
  options(skimr_strip_metadata = FALSE)
  options(digits=2)
  options(scipen=99)
  
  # Numeric report <- Get summary of data frame --------------------------------
  
    # data frame stats
    dfStats.num <- data.frame(Num_Numeric_Variables = ncol(df %>% select_if(is.numeric)),
                              Total_Observations    = nrow(df) )
    
    # Now see individual column statistics
    dfColStats.num <- dataReport.numeric %>% 
      dplyr::select(-type, -hist)
    
  
  # Factor report <- Get summary of data frame --------------------------------
  
    # Get summary of data frame
    dfStats.factor <- data.frame(Num_Factor_Variables = ncol(df %>% select_if(is.factor)),
                                 Total_Observations   = nrow(df) )
    
    # Now see individual column statistics
    dfColStats.factor <- dataReport.factor  %>% 
      dplyr::select(-type, -ordered) 
    
    
  # Return the data frames
  return(list('dfStats.num'       = dfStats.num,    
              'dfColStats.num'    = dfColStats.num,
              'dfStats.factor'    = dfStats.factor, 
              'dfColStats.factor' = dfColStats.factor))
}
```


## Numeric Data Quality Report

```{r}
# Get the factor and numeric reports
initialReport <- dataQualityReport(df.train.base)

# Numeric data frame stats
initialReport$dfStats.num %>% kable()

# Numeric column stats
initialReport$dfColStats.num %>%
  kable() %>% kable_styling(font_size=7, latex_options = 'HOLD_position') # numeric data

```


## Factor Data Quality Report

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# factor data frame stats
initialReport$dfStats.factor %>% kable()

# factor column stats
initialReport$dfColStats.factor %>%
  kable() %>% kable_styling(font_size=7, latex_options = 'HOLD_position') # numeric data
```


\newpage

## Expected Approach for Cleaning Data

### Missingingness
* To handle missingness, we will likely take the following approach for `numeric` and `factor` data:  
  - `Numeric`: Impute missing values using predictive mean matching with the `mice` package  
  - `Factor`: Leverage k-nearest neighbors to impute missing factor data. 
  This is likely possible because there is not a significant portion of the factor 
  data that is missing, so it should not be computationally extensive.    

### Outliers
1. We will prioritize limiting outliers of the target variable.  
2. We will also analyze each numeric independent variable to discover any outliers.
If there are few outliers, then we will likely omit that data. If outliers persist in
a large portion of the data, then we will limit the removal of outlying data.

### Skews

### Target Variable  
* The below exploratory analysis shows that the $Target\_Variable$ is skewed.  
* However, see that the $\log (Target\_Variable)$ is close to being normal, so 
we will need to transform this data.

### Other Numeric Predictors
If other numeric variables are skewed (within the test and train data),
then we will likely use the `boxcox` function to normalize the test and training variables
associated. We will test for skewness in in numeric data using the `skewness` 
function in the `moments` package.

### Factors
Since the factor data contains few unique values, we will not need to factor lump the data.
If there was many unique values in related variables, then we would factor lump to 
help fit the models more efficiently.

\newpage

# Exploratory Analysis and Visualizations

## Exploring the Target Variable

```{r}
# names(df.train.base)

# Histogram of target variable and log(Target_variable + 1)
ggplot(df.train.base, aes(x=Target_Variable)) + 
  geom_histogram(aes(y=..density..), color="black", fill="deepskyblue") +
  geom_density(alpha=.6, fill="#FF6666") + 
  ggtitle("Histogram of Target_Variable")


ggplot(df.train.base, aes(x=log(Target_Variable + 1))) + 
  geom_histogram(aes(y=..density..), color="black", fill="deepskyblue") +
  geom_density(alpha=.6, fill="#FF6666") +
  ggtitle("Histogram of log(Target_Variable + 1)")
```

### Target Variable  
* The below exploratory analysis shows that the $Target\_Variable$ is skewed.  
* However, see that the $\log (Target\_Variable)$ is close to being normal, so 
we will need to transform this data.


\newpage  

```{r, echo=FALSE, results='hide'}
#Correlation between the numeric variables

# ggcorr(df.train.base, geom='blank', label=T, label_size=3, hjust=1,
#        size=3, layout.exp=2) +
#   geom_point(size = 4, aes(color = coefficient > 0, alpha = abs(coefficient) >= 0.5)) +
#   scale_alpha_manual(values = c("TRUE" = 0.25, "FALSE" = 0)) +
#   guides(color = F, alpha = F)
# 
# 
# cor(df.train.base.numeric) %>% knitr::kable()
# 
# # Correlation between factor  variables 
# aov.dat <- aov(Target_Variable ~ Delivery_Person_Id+Weather_Conditions+
#                  Road_Traffic_Density+Vehicle_Condition+Type_Of_Order+
#                  Type_Of_Vehicle+Multiple_Deliveries+Festival+City, 
#                data=df.train.base)
# summary(aov.dat)
```

## Visualizations of interactions between Target variable and factor variables

```{r}

# Violin chart of Multiple_Deliveries vs Target_Variable
ggplot(df.train.base, aes(x=Multiple_Deliveries, y=Target_Variable, fill=Multiple_Deliveries)) +
  geom_violin() +
  coord_flip() + # This switch X and Y axis and allows to get the horizontal version
  theme(legend.position="none") +
  ggtitle("Violin chart: # of multiple deliveries vs target variable") +
  xlab("# Of multiple deliveries")+
  ylab('Time Taken to Deliver')+
  theme(text=element_text(size=10),
        plot.title.position = 'plot',
        plot.title = element_text(hjust = 0.5))


# Violin chart of City vs Target_Variable
ggplot(df.train.base, aes(x=City, y=Target_Variable, fill=City)) +
    geom_violin() +
    coord_flip() +
    theme(legend.position="none") +
    ggtitle("Violin chart: City vs target variable") +
    xlab("City") +
    ylab("Time Taken to Deliver")+
  theme(text=element_text(size=10),
        plot.title.position = 'plot',
        plot.title = element_text(hjust = 0.5))


# Boxplot of Vehicle_Condition vs Target_Variable. The Red dot is the mean of the group
# There was so much overlap with the violin plot, the boxplot is a better visual
ggplot(df.train.base, aes(x=Vehicle_Condition, y=Target_Variable, fill=Vehicle_Condition)) +
    geom_boxplot(alpha=0.5) +
    stat_summary(fun=mean, geom="point", shape=20, size=6, color="red", fill="red") +
    theme(legend.position="none") +
    scale_fill_brewer(palette="Set1")+
  ggtitle("Boxplot chart: Vehicle Condition vs target variable") +
    xlab("Vehicle Condition") +
    ylab("Time Taken to Deliver")+
  theme(text=element_text(size=10),
        plot.title.position = 'plot',
        plot.title = element_text(hjust = 0.5))
  

```

 The violin chart of "# of multiple deliveries vs target_variable"
 shows that the more deliveries you make, the more of the target_variable
 you make

 The Violin chart of "City vs target_variable" indicates that Semi-Urban areas 
 have the highest Target variable, and it is a slim distribution, meaning 
 it does not vary as much as the other distributions

 The Boxplot of "Vehicle_Condition vs Target_Variable" indicates that vehicle condition 
 0 and 3 have a higher target variable


\newpage

## Visualizations of interactions between Target variable and numeric variables

```{r}

# Scatterplot of Delivery_Person_Ratings vs Target_Variable
# Multiple deliveries is being used to determine the color of a point to look for patterns
ggplot(df.train.base, aes(x=Delivery_Person_Ratings, y=Target_Variable, color=Multiple_Deliveries)) + 
  geom_point(alpha = 0.2, position=position_jitter(height=.5, width=.5)) +
  labs(title = "Scatterplot of Delivery Person's Ratings vs Target Variable",
       x="Ratings of Delivery Persons",
       y="Time Taken to Deliver",
       col="Multiple Deliveries")+
  theme(text=element_text(size=10),
        plot.title.position = 'plot',
        plot.title = element_text(hjust = 0.5))

# Scatterplot of Delivery_Person_Ratings vs Target_Variable
# City is being used to determine the color of a point to look for patterns
ggplot(df.train.base, aes(x=Delivery_Person_Ratings, y=Target_Variable, color=City)) + 
  geom_point(alpha = 0.2, position=position_jitter(height=.5, width=.5)) +
  labs(title = "Scatterplot of Delivery Person's Ratings vs Target Variable",
       x="Ratings of Delivery Persons",
       y="Time Taken to Deliver",
       col="City")+
  theme(text=element_text(size=10),
        plot.title.position = 'plot',
        plot.title = element_text(hjust = 0.5))


```

 This scatterplot is great at showing the relation ship of multiple deliveries
 and the increasing amount of the target_variable. Those with more deliveries,
 tend to have a higher target_variable. Another scatterplot was drawn with the
 city being used as the determination for color. This visual shows Semi-Urban
 having the highest amount of the target variable, followed by Urban and Metropolitian




<!-- INSERT HERE  -->
<!-- -- -->
<!-- -- -->
<!-- -- -->
<!-- -- -->
<!-- -- -->
<!-- -- -->
<!-- -- -->
<!-- -- -->
<!-- -- -->
<!-- -- -->
<!-- -- -->
<!-- -- -->
<!-- -- -->
<!-- -- -->
<!-- -- -->
<!-- -- -->
<!-- -- -->
<!-- -- -->
<!-- -- -->


  