---
title:    "ISE 5103 Intelligent Data Analytics"
subtitle: "Homework 6 - Modeling Competition"
author:   "Daniel Carpenter, Sonaxy Mohanty, & Zachary Knepp"
date:     "October 2022"
output: 
  pdf_document:
    toc: true
    toc_depth: 3
    highlight: arrow
    latex_engine: xelatex
  # github_document:
  #   toc: yes
  #   toc_depth: 2
urlcolor: blue
cache: true
fig.width: 7
fig.height: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage

```{r error=FALSE, message=FALSE, warning=FALSE}
# Packages --------

# Data Wrangling
library(tidyverse)
library(skimr)
library(lubridate) # dates

# Modeling
library(MASS)
library(caret) # Modeling variants like SVM
library(earth) # Modeling with Mars
library(pls)   # Modeling with PLS
library(glmnet) # Modeling with LASSO

# Aesthetics
library(knitr)
library(cowplot)  # multiple ggplots on one plot with plot_grid()
library(scales)
library(kableExtra)
library(ggplot2)
library(inspectdf)

#Hold-out Validation
library(caTools)

#Data Correlation
library(GGally)
library(regclass)

#RMSE Calculation
library(Metrics)

#p-value for OLS model
library(broom)

#ncvTest
library(car)
```

## General Data Prep
> For general data preparation, please see conceptual steps below. See `.rmd` file for detailed code.

### Read Training Data
Clean data to ensure each read variable has the correct data type (factor, numeric, Date, etc.)
```{r, echo=FALSE}
# Convert all character data to factor
df.train.base <- read.csv('Train.csv', stringsAsFactors = TRUE)
df.test <- read.csv('Test.csv', stringsAsFactors = TRUE)


# convert the ""'s to NA
df.train.base[df.train.base == ""] <- NA

# Clean data
df.train.base <- df.train.base %>% 
  
  # Ensure boolean variables are numeric
  mutate(adwordsClickInfo.isVideoAd = as.numeric(adwordsClickInfo.isVideoAd) ) %>%
  
  # Make sure dates are dates
  mutate(date = as.Date(date),
         visitStartTime = as_datetime(visitStartTime)
         ) %>%

  # Ensure factor are factors
  mutate(custId       = as.factor(custId),
         sessionId    = as.factor(sessionId),
         isTrueDirect = as.factor(isTrueDirect),
         newVisits    = as.factor(if_else(is.na(newVisits), 0, 1) ),
         bounces      = as.factor(if_else(is.na(bounces),   0, 1)   ),
         adwordsClickInfo.page      = as.factor(adwordsClickInfo.page),
         adwordsClickInfo.isVideoAd = as.factor(adwordsClickInfo.isVideoAd)
         ) %>%
  
  dplyr::select(-c(
    isMobile # This is contained in deviceCategory
    
  ))

#view(df.train.base)
```

### Create `numeric` and `factor` *base* `data frames`

Make data set of `numeric` variables called `df.train.base.numeric`
```{r, echo=FALSE}
df.train.base.numeric <- df.train.base %>%

  # selecting all the numeric data
  dplyr::select_if(is.numeric) %>%

  # converting the data frame to tibble
  as_tibble()
```

Make data set of `factor` variables called `df.train.base.factor`
```{r, echo=FALSE}
df.train.base.factor <- df.train.base %>%

  #selecting all the numeric data
  dplyr::select_if(is.factor) %>%

  #converting the data frame to tibble
  as_tibble()
```


## `(a, i)` - Data Understanding
> Create a data quality report of `numeric` and `factor` data  
> Created function called `dataQualityReport()` to create factor and numeric QA report

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Function for data report
dataQualityReport <- function(df) {
  
  # Function to remove any columns with NA
  removeColsWithNA <- function(df) {
    return( df[ , colSums(is.na(df)) == 0] )
  }
  
  # Create Comprehensive data report using skimr package
  # This is done a bit piece-wise because PDF latex does not like the skimr package
  # Very much. So Instead of printing `skim(df)`, I have to pull the contents manually
  # Unfortunately. This is not an issue with html typically.
  dataReport <- skim(df) %>%
    rename_all(~str_replace(.,"skim_","")) %>%
    arrange(type, desc(complete_rate) ) # sort data 
  
  # Filter to the class types
  dataReport.numeric <- dataReport %>% filter(type == 'numeric') # numeric data
  dataReport.factor  <- dataReport %>% filter(type == 'factor' ) # factor  data
  
  # Remove columns that do not apply to this type of data -----------------------
  
  ## numeric data
  dataReport.numeric <- removeColsWithNA(dataReport.numeric)  %>%
    
    # Clean column names by removing numeric prefix, 
    rename_all(~str_replace(.,"numeric.","")) 
    
  ## factor  data
  dataReport.factor  <- removeColsWithNA(dataReport.factor ) %>%
  
    # Clean column names by removing factor  prefix
    rename_all(~str_replace(.,"factor.",""))  
  
  
  # Set up options for Display the reports
  options(skimr_strip_metadata = FALSE)
  options(digits=2)
  options(scipen=99)
  
  # Numeric report <- Get summary of data frame --------------------------------
  
    # data frame stats
    dfStats.num <- data.frame(Num_Numeric_Variables = ncol(df %>% select_if(is.numeric)),
                              Total_Observations    = nrow(df) )
    
    # Now see individual column statistics
    dfColStats.num <- dataReport.numeric %>% 
      dplyr::select(-type, -hist)
    
  
  # Factor report <- Get summary of data frame --------------------------------
  
    # Get summary of data frame
    dfStats.factor <- data.frame(Num_Factor_Variables = ncol(df %>% select_if(is.factor)),
                                 Total_Observations   = nrow(df) )
    
    # Now see individual column statistics
    dfColStats.factor <- dataReport.factor  %>% 
      dplyr::select(-type, -ordered) 
    
    
  # Return the data frames
  return(list('dfStats.num'       = dfStats.num,    
              'dfColStats.num'    = dfColStats.num,
              'dfStats.factor'    = dfStats.factor, 
              'dfColStats.factor' = dfColStats.factor))
}
```

### Numeric Data Quality Report
* `pageviews` has some null values, but there are an insignificant amount, so we will just drop those rows.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Get the factor and numeric reports
initialReport <- dataQualityReport(df.train.base)

# Numeric data frame stats
initialReport$dfStats.num %>% kable()

# Numeric column stats
initialReport$dfColStats.num %>%
  kable() %>% kable_styling(font_size=7, latex_options = 'HOLD_position') # numeric data
```

\newpage

### Factor Data Quality Report
* Location data unknown, so add an `Unknown` label for `null` values
* Appears that few people use website from the ads, which cause many null values. See more details below.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# factor data frame stats
initialReport$dfStats.factor %>% kable()

# factor column stats
initialReport$dfColStats.factor %>%
  kable() %>% kable_styling(font_size=7, latex_options = 'HOLD_position') # numeric data
```

\newpage

### Exploratory Analysis

```{r}

# Transforming the revenue

# transformed_revenue <- df.train.base %>%
#   group_by(custId) %>%
#   summarise(targetRevenue = log(sum(revenue)) + 1) %>%
#   ungroup()


#aggregate revenue
CustRev <- stats::aggregate(df.train.base$revenue, 
                     by=list(df.train.base$custId),
                     FUN = sum,
                     na.rm = TRUE)

#renaming fields
names(CustRev) <- c('custId', 'totalRevenue')

#merging datasets
df.train.merge <- merge(df.train.base, CustRev, by='custId')

#applying transformation
df.train.merge$totalRevenue <- df.train.merge$totalRevenue + 1
df.train.merge$totalRevenue <- log(df.train.merge$totalRevenue)



```


#### Analysis 1: 
  
*   Checking the distribution of the transformation of the aggregrate customer-level sales value
based on the natural log:  

```{r}

hist(df.train.merge$totalRevenue,
     col = 'skyblue4',
     main = 'Distribution of Target Revenue for each customer',
     xlab = 'Target Revenue')
```
  
*   We can see that the transformed revenue doesn't look like a normal distribution with a spike at 0 revenue which means it can be an outlier.  

#### Analysis 2: 
  
*   Correlation between features in the dataset  

```{r}
df.train.merge %>%
  ggplot(aes(x = fct_reorder(channelGrouping, desc(totalRevenue) ),
             y = totalRevenue) ) +
# Boxplots
  geom_boxplot(aes(color = channelGrouping), fill = 'lightsteelblue1', alpha = 0.7) +
  coord_flip() +
# Theme, y scale format, and labels
  theme_minimal() + 
  theme(panel.grid.major.x = element_blank()) +
  #scale_y_continuous(labels = comma) +
  labs(title = 'Distribution of Transformed Revenue by Different Online Store Channels',
       subtitle = 'Ordered Descending by Transformed Revenue Generated by Channels',
       x = 'Channels Used by Customers for Online Store',
       y = 'Transformed Revenue Generated')
```


\newpage

## `(a, ii)` - Data Preparation
> For general data preparation, please see conceptual steps below. See `.rmd` file for detailed code.

### Clean up Null Data

See that when `region` is `Osaka Prefecture` and `city` is `Osaka` some location details are `NULL` 

* Implication: the other fields can be manually set to correct values based on region and city criteria  
  
* So, set `location related` null fields to `know` description for the above `region` and `city` condition

```{r, echo=FALSE, results='hide'}

# df.train.base[!complete.cases(df.train.base$continent), ] %>%
#   distinct(continent, subContinent, country, region, metro, city)
# 
# 
# df.train.base %>%
#   filter(region == 'Osaka Prefecture') %>%
#   distinct(continent, subContinent, country, metro, city, region)

df.train <- df.train.merge


df.train$continent[is.na(df.train$continent) &
           df.train$region == 'Osaka Prefecture'] <- 'Asia'

# df.train %>%
#   filter(region == 'Osaka Prefecture' & city == 'Osaka') %>%
#   distinct(subContinent)

df.train$subContinent[is.na(df.train$subContinent) &
           df.train$region == 'Osaka Prefecture' &
             df.train$city == 'Osaka'] <- 'Eastern Asia'

# df.train %>%
#   filter(region == 'Osaka Prefecture' & city == 'Osaka') %>%
#   distinct(country)

df.train$country[is.na(df.train$country) &
           df.train$region == 'Osaka Prefecture' &
             df.train$city == 'Osaka'] <- 'Japan'
  
# df.train %>%
#   filter(region == 'Osaka Prefecture' & city == 'Osaka') %>%
#   distinct(metro)

# df.train %>%
#   filter(metro == 'JP_KINKI')

df.train$metro[is.na(df.train$metro) &
           df.train$region == 'Osaka Prefecture' &
             df.train$city == 'Osaka'] <- 'JP_KINKI'

# df.train %>%
#   filter(region == 'Osaka Prefecture' & city == 'Osaka')

```
  
See that when `continent` is `null`, then other `location` related fields are also null  

* Implication: these other fields depend on the `continent` variable  

* So, set `location related` null fields to `Unknow` description 

```{r, echo=FALSE, results='hide'}

# df.train[!complete.cases(df.train$continent), ] %>%
#   distinct(continent, subContinent, country, region, metro, city)

```

```{r, echo=FALSE}
#UNKNOWN_TEXT = 'Unknown'

# If null in location data, then 'Unknown' location
df.train <- df.train %>%
  mutate_at(
    # Only mutate these location variables
    vars(continent:city), 
    
    # Apply function rename null values to Unknown
    list(~ as.factor(ifelse(is.na(.), 'Unknown', .) ) ) 
  )

#df.train%>% filter(continent == 'Unknown')
```

```{r, eval=FALSE, echo=FALSE}
# Check the data set - see that most of the ad data is now cleaned.
# report2 <- dataQualityReport(df.train)
# report2$dfColStats.factor %>% kable()
```

```{r, echo=FALSE, results='hide'}

# df.train %>%
#   distinct(medium)
# 
# df.train[!complete.cases(df.train$medium), ] %>%
#   distinct(medium, campaign,keyword, referralPath, 
#            adContent, adwordsClickInfo.page, adwordsClickInfo.slot, 
#            adwordsClickInfo.gclId, adwordsClickInfo.adNetworkType, adwordsClickInfo.isVideoAd)

```
  
See that when `medium` is `null`, then other `ad`, `keyword` and `campaign` related fields are (mostly) null  

* Implication: these other fields depend on the `medium` variable

* So, set these null fields to `None` description, since a null value indicates
the user did not has `no traffic source`  

```{r, echo=FALSE}
# NO_MEDIUM_TEXT = 'No traffic source '
#NONE_TEXT = 'No traffic source '

# Now clean up the data in the main data frame `df.train`
# by setting null values to "No taffic source" if there is no medium
# Applies to "ad*", keyword, and campaign, referralPath, medium variables
df.train <- df.train %>%
  mutate_at(
    # Only mutate the variables starting with ad, THEN the campaign variable
    vars(starts_with('ad'), keyword, campaign, referralPath, medium), 
    
    # Apply function rename set the campaign text if campaign is null
    list(~ as.factor(ifelse(is.na(medium), 'No traffic source ', .) ) ) 
  ) 

#df.train %>% filter(keyword == 'No traffic source ')
```

```{r, eval=FALSE, echo=FALSE}
# Check the data set - see that most of the ad data is now cleaned.
# report3 <- dataQualityReport(df.train)
# report3$dfColStats.factor %>% kable()
```

```{r, echo=FALSE, results='hide'}

# df.train %>%
#   distinct(campaign)
# 
# df.train[!complete.cases(df.train$campaign), ] %>%
#   distinct(campaign, adwordsClickInfo.page, adwordsClickInfo.slot, 
#            adwordsClickInfo.adNetworkType, adwordsClickInfo.isVideoAd)

```

See that when `campaign` is `null`, then some `ad` related fields are (mostly) null  

* Implication: these other fields depend on the `campaign` variable

* So, set `adwordsClickInfo.page` null fields to `None` description, since a null value indicates
the user did not come using an advertisement  

```{r, echo=FALSE}
#NO_CAMPAIGN_TEXT = 'No Campaign'
#NONE_TEXT = 'No Campaign'

# Now clean up the data in the main data frame `df.train`
# by setting null values to "None" if there is no campaign.
# Applies to "ad*", keyword, and campaign variables
df.train <- df.train %>%
  mutate_at(
    # Only mutate the variables starting with ad, THEN the campaign variable
    vars(adwordsClickInfo.page, adwordsClickInfo.slot, adwordsClickInfo.adNetworkType, adwordsClickInfo.isVideoAd, campaign), 
    
    # Apply function rename set the campaign text if campaign is null
    list(~ as.factor(ifelse(is.na(campaign), 'No Campaign', .) ) ) 
  ) 

```

```{r, eval=FALSE, echo=FALSE}
# Check the data set - see that most of the ad data is now cleaned.
# report4 <- dataQualityReport(df.train)
# report4$dfColStats.factor %>% kable()
```

```{r, echo=FALSE, results='hide'}
# df.train[!complete.cases(df.train$keyword), ] %>%
#   distinct(adContent, adwordsClickInfo.adNetworkType, adwordsClickInfo.isVideoAd, keyword)
```

Similar to campaign, whenever `keyword` is NA, some `ads` is null  
  
```{r, echo=FALSE}
#NO_KEYWORD_TEXT = 'No Keyword'

# Now clean up the data in the main data frame `df.train`
# by setting null values to "No Keyword" if there is no keyword
# Applies to some "ad*", and keyword variables
df.train <- df.train %>%
  mutate_at(
    # Only mutate the variables starting with ad, THEN the keyword variable
    vars(adContent, adwordsClickInfo.adNetworkType, adwordsClickInfo.isVideoAd, keyword), 
    
    # Apply function rename set the campaign text if campaign is null
    list(~ as.factor(ifelse(is.na(keyword), 'No Keyword', .) ) ) 
  ) 

#view(df.train)
```

```{r, eval=FALSE, echo=FALSE}
# Check the data set - see that most of the ad data is now cleaned.
# report5 <- dataQualityReport(df.train)
# report5$dfColStats.factor %>% kable()
```

```{r, echo=FALSE, results='hide'}

# df.train[!complete.cases(df.train$adContent), ] %>%
#   distinct(adContent, referralPath)

```

Similar to the campaign data, if the `adContent` is null, label as `No Ad`. 
  
*   Implications: If there is no ad Content of the traffic source then there is no no referral path  

```{r, echo=FALSE}
#NONE_TEXT = 'No Ad'

# If the `adContent` is null, label as `None`
df.train <- df.train %>%
  mutate_at(
    # Only mutate the referral path
    vars(referralPath, adContent), 
    
    # Apply function rename set the referral to none
    list(~ as.factor(ifelse(is.na(adContent), 'No Ad', .) ) ) 
  )
```

```{r, eval=FALSE, echo=FALSE}
# Check the data set - see that most of the ad data is now cleaned.
# report6 <- dataQualityReport(df.train)
# report6$dfColStats.factor %>% kable()
```

```{r, echo=FALSE, results='hide'}

# df.train[!complete.cases(df.train$adwordsClickInfo.adNetworkType), ] %>%
#   distinct(adwordsClickInfo.adNetworkType,adwordsClickInfo.page, adwordsClickInfo.slot,
#            adwordsClickInfo.gclId, adwordsClickInfo.adNetworkType, adwordsClickInfo.isVideoAd)

#referralPath,adwordsClickInfo.page, adwordsClickInfo.slot,  adwordsClickInfo.gclId,adwordsClickInfo.adNetworkType,adwordsClickInfo.isVideoAd

```

Similar to the campaign data, if the `adwordsClickInfo.adNetworkType` is null, then all `ad` related variables are also `NULL`. 
  
*   Implications: If there is no ad search then customer didn't see any ad.  

```{r, echo=FALSE}
#NONE_TEXT = 'No Ad Network'

# If the `adwordsClickInfo.adNetworkType` is null, label as `No Ad Network`
df.train <- df.train %>%
  mutate_at(
    # Only mutate the referral path
    vars(adwordsClickInfo.page, adwordsClickInfo.slot, adwordsClickInfo.gclId,
         adwordsClickInfo.isVideoAd, adwordsClickInfo.adNetworkType), 
    
    # Apply function rename set the referral to none
    list(~ as.factor(ifelse(is.na(adwordsClickInfo.adNetworkType), 'No Ad Network', .) ) ) 
  )
```

```{r, eval=FALSE, echo=FALSE}
# Check the data set - see that most of the ad data is now cleaned.
# report7 <- dataQualityReport(df.train)
# report7$dfColStats.factor %>% kable()
```

```{r, echo=FALSE, results='hide'}

# df.train[!complete.cases(df.train$adwordsClickInfo.page), ] %>%
#   distinct(adwordsClickInfo.page, referralPath, adwordsClickInfo.slot, adwordsClickInfo.gclId)

#adwordsClickInfo.page, referralPath, adwordsClickInfo.slot,  adwordsClickInfo.gclId

```

Similar to the adwordsClickInfo.adNetworkType data, if the `adwordsClickInfo.page` is null, then some `ad` related variables are also `NULL` and there is no referral source. 
  
*   Implications: If there is no ad published on a page then customer didn't see any ad.  

```{r, echo=FALSE}
#NONE_TEXT = 'No Ad Page'

# If the `adwordsClickInfo.page` is null, label as `No Ad Page`
df.train <- df.train %>%
  mutate_at(
    # Only mutate the referral path
    vars(referralPath, adwordsClickInfo.slot, adwordsClickInfo.gclId, adwordsClickInfo.page), 
    
    # Apply function rename set the referral to none
    list(~ as.factor(ifelse(is.na(adwordsClickInfo.page), 'No Ad Page', .) ) ) 
  )
```

```{r, eval=FALSE, echo=FALSE}
# Check the data set - see that most of the ad data is now cleaned.
# report8 <- dataQualityReport(df.train)
# report8$dfColStats.factor %>% kable()
```

```{r, echo=FALSE, results='hide'}

df.train[!complete.cases(df.train$networkDomain), ] %>%
  distinct(topLevelDomain)

#topLevelDomain

```

If `network domain` is `NULL` then all the related domains are also NULL. 
  

```{r, echo=FALSE}
#NONE_TEXT = 'No Domain'

# If the `network domain` is null, label as `No Domain`
df.train <- df.train %>%
  mutate_at(
    # Only mutate the referral path
    vars(networkDomain:topLevelDomain), 
    
    # Apply function rename set the referral to none
    list(~ as.factor(ifelse(is.na(.), 'No Domain', .) ) ) 
  )
```

```{r, eval=FALSE, echo=FALSE}
# Check the data set - see that most of the ad data is now cleaned.
# report9 <- dataQualityReport(df.train)
# report9$dfColStats.factor %>% kable()
```

```{r, echo=FALSE, results='hide'}
# 
# df.train[!complete.cases(df.train$adwordsClickInfo.gclId), ] %>%
#   distinct(referralPath, adwordsClickInfo.gclId)

```
  
Setting `referralPath` for NAs. 

```{r, echo=FALSE}
#NONE_TEXT = 'No Referrer'

# If the `network domain` is null, label as `No Domain`
df.train <- df.train %>%
  mutate_at(
    # Only mutate the referral path
    vars(referralPath), 
    
    # Apply function rename set the referral to none
    list(~ as.factor(ifelse(is.na(referralPath), 'No Referrer', .) ) ) 
  )
```

```{r, eval=FALSE, echo=FALSE}
# Check the data set - see that most of the ad data is now cleaned.
# report10 <- dataQualityReport(df.train)
# report10$dfColStats.factor %>% kable()
```

Setting `adwordsClickInfo.gclId` for NAs. 

```{r, echo=FALSE}
#NONE_TEXT = 'No Google Click ID'

# If the `network domain` is null, label as `No Domain`
df.train <- df.train %>%
  mutate_at(
    # Only mutate the referral path
    vars(adwordsClickInfo.gclId), 
    
    # Apply function rename set the referral to none
    list(~ as.factor(ifelse(is.na(adwordsClickInfo.gclId), 'No Google Click ID', .) ) ) 
  )

#view(df.train)
```

```{r, eval=FALSE, echo=FALSE}
# Check the data set - see that most of the ad data is now cleaned.
# report11 <- dataQualityReport(df.train)
# report11$dfColStats.factor %>% kable()
```

Now we have very few null values rows. Let's simply remove them. See below for how many.
```{r, echo=FALSE}

df.train <- df.train
#deselecting the browser and source columns
# df.train <- df.train %>%
#   dplyr::select(!c(browser, operatingSystem))

# Number of rows with any nulls
numRowsWithNulls <- nrow(df.train[!complete.cases(df.train), ])

# Output text
paste('There are', numRowsWithNulls, 'rows with nulls')
paste0('That equates to ', round(numRowsWithNulls / nrow(df.train)* 100, 1), '% rows with nulls')

# Drop the rows
df.train <- df.train %>% drop_na()
paste('Total Rows Remaining:', nrow(df.train))
```

```{r, eval=FALSE, echo=FALSE}
# Check the data set - see that most of the ad data is now cleaned.
# report12 <- dataQualityReport(df.train)
# report12$dfColStats.factor %>% kable()
```


```{r, echo=FALSE}
df.train.clean <- df.train

# Make data set of `factor` variables called `df.train.base.factor`
df.train.factor <- df.train %>%

  # selecting all the numeric data
  dplyr::select_if(is.factor) %>%

  # converting the data frame to tibble
  as_tibble()

# Get list of factors and the number of unique values
#factorCols <- 
  as.data.frame(t(df.train.factor %>% summarise_all(n_distinct))) %>%
  kable()

# # We are going to factor collapse factor columns with more than 4 columns
# # So there will be 5 of the original, and 1 containing 'other'
# # This is the threshold
# factorThreshold = 5
# 
# # Get a list of the factors we are going to collapse
# colsWithManyFactors <- rownames(factorCols %>% filter(V1 > factorThreshold))
# 
# # Show a summary of how many factors will be collapsed
# numberOfColsWithManyFactors = length(colsWithManyFactors)
# paste('Before cleaning, there are', numberOfColsWithManyFactors, 'factor columns with more than', 
#       factorThreshold, 'unique values')
# 
# # Collapse the affected factors in the original data (the one that already has imputation)
# ## for each factor column that we are about to collapse
# # The third column is omits the cutstomer ID and session ID
# FIRST_NON_CUST_SESSION_IDX = 3
# for (collapsedColNum in FIRST_NON_CUST_SESSION_IDX:numberOfColsWithManyFactors) {
#   
#   # The name of the column with null values
#   nameOfThisColumn <- colsWithManyFactors[collapsedColNum]
#   
#   # Get the actual data of the column with nulls
#   colWithManyFactors <- df.train[, nameOfThisColumn]
#   
#   # lumps all levels except for the n most frequent 
#   df.train.clean[, nameOfThisColumn] <- fct_lump_n(colWithManyFactors, 
#                                                    n=factorThreshold)
# }
# Check to see if the factor lumping worked
#factorColsCleaned <- 
  t(df.train.clean %>%
                       select_if(is.factor) %>%
                       summarise_all(n_distinct))
# # paste('After cleaning, there are', sum(factorColsCleaned > factorThreshold + 1, na.rm = TRUE), 
# #       "columns with more than", factorThreshold + 1, "unique values (omitting NA's)") 
```

\newpage

## `(a, iii)` - Modeling

### OLS Model
#### Model Setup
#### Fit the Model
#### View and Interpret Results

```{r}
#Cleaning up some  variables no longer needed
rm(CustRev)
rm(numRowsWithNulls)
rm(df.train.base)
rm(df.train.base.factor)
rm(df.train.base.numeric)
rm(df.train.merge)
```

```{r}

ols <- lm(totalRevenue ~ pageviews+visitNumber+
            referralPath+deviceCategory+
            region+metro+city, data = 
            df.train.clean)

summary(ols)
plot(ols)
```

\newpage

### Model 2: PCR Mixed with SVM

#### Model Setup
* Uses `numeric` data for Principal Component Analysis
  - Data includes outliers
  - Chose number of PC's that explain 70% of the variation. This is just a general judgement call to keep the number of principal components low.

* Then appends the `factor` columns *without `NULL` values* and `Revenue` to the data
* Finally, uses `stepAIC()` to best model data
* See interpretation at end

```{r}
# Get cleaned `numeric` and `factor` `data frames`
# After cleaning, two data sets that contain..

## Numeric data 
df.train.clean.numeric <- df.train.clean %>% select_if(is.numeric)

## Factors 
df.train.clean.factor  <- df.train.clean %>% dplyr::select(where(is.factor))
```

```{r}
# Perform PCA
# Principal component analysis on numeric data
#df.train.clean.numeric[!complete.cases(df.train.clean.numeric),]

pc.train <- prcomp(df.train.clean.numeric %>% dplyr::select(-c(revenue, totalRevenue)), # do not include response var
                   center = TRUE, # Mean centered
                   scale  = TRUE  # Z-Score standardized
                   )
# See first 10 cumulative proportions
pc.train.summary <- summary(pc.train)
pc.train.summary
```


```{r}
# Now we choose number of PC's that explain 75% of the variation
# Note this threshold is just a judgement call. No significance behind 75%
cumPropThreshold = 0.70 # The threshold
numPCs <- sum(pc.train.summary$importance['Cumulative Proportion', ] < cumPropThreshold)
paste0('There are ', numPCs, ' principal components that explain up to ', cumPropThreshold*100,
       '% of the variation in the data')
chosenPCs <- as.data.frame(pc.train$x[, 1:numPCs])
```

```{r}
#removing some unwanted variables
rm(cumPropThreshold)
rm(numPCs)
rm(pc.train)
rm(pc.train.summary)
```




Join on the `factor` data and `revenue`
```{r}
# df.svm <- cbind(revenue = df.train.clean.numeric$totalRevenue,
#                 chosenPCs,
#                 df.train.clean.factor)  %>%
#   
#   dplyr::select(-c(
#     # Remove ID cols since will overfit model
#     sessionId, custId, 
#     
#     # Remove ad content for the time being
#     starts_with('ad')    
#     ) )

df.svm <- data.frame(df.train.clean, chosenPCs)
```

Model controls
```{r}
ctrl <- trainControl(method  = "repeatedcv", 
                     number  = 5, # 5 fold cross validation
                     repeats = 0  # 2 repeats
                     )
```


#### Fit the Model
* SVM model containing:
  - Principal components explaining 70% of variation in `numeric` data
  - Non-null `factor` data
  - *Predicted variable:* `revenue`


```{r svm, warning=FALSE, message=FALSE}


# Train and tune the SVM

fit.svm <- train(data = df.svm,
                 totalRevenue ~ PC1+PC2+referralPath+bounces+deviceCategory+
            region+metro+city+pageviews,
                 method     = "svmLinear",         # Radial kernel
                 tuneLength = 9,                   # 9 values of the cost function
                 preProc    = c("center","scale"), # Center and scale data
                 trControl  = ctrl)

#memory.limit(10*7988)
```

#### View and Interpret Results

```{r, fig.height=4, echo=FALSE}
# Final model?
fit.svm$finalModel

# How do the predicted vs. Actuals Compare?
# predictedVsObserved(observed  = log(df.svm$SalePrice),
#                     predicted = predict(fit.svm, df.svm),
#                     modelName = 'SVM')

# Gather key diagnostics for summary table
# Get the RMSE and R Squared of the model
hyperparameters.svm = list('C' = fit.svm[["finalModel"]]@param[["C"]],
                           'Epsilon' = fit.svm[["finalModel"]]@param[["epsilon"]])

keyDiagnostics.svm <- data.frame(Model    = 'SVM',
                                 Notes    = 'caret and svmRadial',
                                 Hyperparameters = paste('C =', hyperparameters.svm$C, ',',
                                                         'Epsilon =', hyperparameters.svm$Epsilon)
                                 )
keyDiagnostics.svm <- cbind(keyDiagnostics.svm,
                            fit.svm$results %>% 
                              filter(C == hyperparameters.svm$C) %>%
                              dplyr::select(RMSE, Rsquared)
                      )
# Show output
keyDiagnostics.svm %>% knitr::kable()
```

\newpage

### Model 3: 
#### Model Setup
#### Fit the Model
#### View and Interpret Results
```{r, echo=FALSE, results='hide'}

pcr.df <- data.frame(df.train.clean, chosenPCs)
  
pcr <- lm(totalRevenue ~ PC1+PC2+referralPath+bounces+deviceCategory+
            region+metro+city+pageviews,
          data=pcr.df)

summary(pcr)

```


\newpage

### Model 4: 
#### Model Setup
#### Fit the Model
#### View and Interpret Results
```{r}
marsFit <- earth(totalRevenue ~ timeSinceLastVisit+pageviews+referralPath+bounces+deviceCategory+visitNumber+region+metro+city,
                 data=df.train.clean,
                 degree=3,nk=50,pmethod="cv",nfold=5,ncross=5)
summary(marsFit)
plot(marsFit)
```


\newpage

### Model 5: 
#### Model Setup
#### Fit the Model
#### View and Interpret Results

```{r, warning=FALSE, message=FALSE}
# Train and tune the SVM

rm(df.train.clean.factor)
rm(df.train.clean.numeric)

fit.lasso <- train(data = df.train.clean, 
                 totalRevenue ~ timeSinceLastVisit+pageviews+referralPath+
                   bounces+deviceCategory+visitNumber+region+metro+city,
                 method     = "glmnet",         # Elastic net
                 preProc    = c("center","scale"), # Center and scale data
                 tuneLength = 5,  #10 values of alpha and 10 lambda values for each
                 trControl  = ctrl)
```  
  
```{r, echo=FALSE}
# Function to get the best hypertuned parameters
get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}
```  
  
```{r, echo=FALSE}
result.lasso <- get_best_result(fit.lasso)
```

\newpage

## `(a, iv)` - Debrief

\newpage

## Summary Table
```{r, echo=FALSE}
# Add the key diagnostics here
rbind(
  keyDiagnostics.svm
  ) %>%
  
  # Round to 4 digits across numeric data
  mutate_if(is.numeric, round, digits = 4) %>%
  
  # Spit out kable table
  kable()
```


